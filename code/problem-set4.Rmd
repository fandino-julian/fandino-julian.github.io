---
title: "Problem Set 4 "
author: "JULIAN FANDIÑO_202021070"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
# Configuración del entorno
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(pacman)
p_load(rio, data.table, tidyverse, sf, rvest, ggplot2, viridis, rmarkdown, dplyr)

```


```{r, eval=TRUE, echo=TRUE, message=FALSE, results='hide'}
url <- "https://eduard-martinez.github.io/pset-4.html"
webpage <- read_html(url)
url_full <- webpage %>% html_nodes("a") %>% html_attr("href") %>% na.omit() %>% unique()
url_full
```

```{r, eval=TRUE, echo=TRUE, message=FALSE, results='hide'}
# 1.2 Filtrar URLs que contienen la palabra "propiedad"
url_subset <- url_full[grepl("propiedad", url_full)]
url_subset
```

```{r,eval=TRUE,echo=TRUE,message=FALSE}
# 1.3 Extracción de las tablas de las URLs filtradas
lista_tablas <- lapply(url_subset, function(link) {
  page <- read_html(link)
  table <- page %>% html_table(fill = TRUE) %>% .[[1]]
  return(table)
})
# 1.4 Combinar las tablas en un solo dataframe
db_house <- rbindlist(lista_tablas, fill = TRUE)
colnames(db_house) <- make.names(colnames(db_house), unique = TRUE)

## Mostrar las primeras filas del dataframe
head(db_house)
## Convertir las columnas lat y lon a valores numéricos
db_house <- db_house %>%
  mutate(lat = as.numeric(lat), lon = as.numeric(lon))

# 2.1 Crear un objeto sf a partir del dataframe
sf_house <- st_as_sf(db_house, coords = c("lon", "lat"), crs = 4326)

# 2.2 Crear un mapa con ggplot2
mapa <- ggplot() +
  geom_sf(data = sf_house, aes(color = price), size = 2) +
  scale_color_viridis() +
  theme_minimal() +
  labs(title = "Mapa de Propiedades", color = "Precio")

## Mostrar el mapa
mapa

# Análisis del Mapa de Propiedades

##El mapa de propiedades muestra la distribución espacial de las propiedades extraídas de las diferentes URLs contenidas en la página web especificada. Cada punto en el mapa representa una propiedad, y el color de los puntos indica el precio de la propiedad, utilizando una escala de colores proporcionada por la función `scale_color_viridis`.

### Observaciones Principales:

##1. **Distribución Geográfica:**
   #- Los puntos en el mapa están distribuidos en varias ubicaciones, lo que indica que las propiedades cubren una amplia área geográfica. Esto puede ayudar a identificar concentraciones de propiedades en ciertas regiones.

##2. **Variación de Precios:**
   #- La escala de colores permite observar la variación de precios entre las propiedades. Los colores más claros indican precios más bajos, mientras que los colores más oscuros representan precios más altos.
   #- Se pueden identificar patrones espaciales en los precios, como áreas donde las propiedades tienden a ser más caras o más baratas.

##3. **Identificación de Áreas Clave:**
  # - Algunas regiones pueden mostrar una alta densidad de propiedades con precios elevados, lo que puede señalar áreas urbanas o barrios exclusivos.
   #- Por otro lado, áreas con muchos puntos de colores claros pueden indicar zonas más asequibles.


## Guardar el mapa como un archivo PDF
ggsave("mapa_propiedades.pdf", mapa)

# Cargar librerías necesarias
library(rvest)
library(dplyr)

# Definir las URLs
urls <- c("https://eduard-martinez.github.io/pset-4.html", 
          "https://fandino-julian.github.io/problem-set4.html")

# Función para extraer y guardar información de una URL en un archivo de texto
save_url_info <- function(url, file_name) {
  webpage <- read_html(url)
  
  # Extraer URLs de la página
  url_full <- webpage %>% html_nodes("a") %>% html_attr("href") %>% na.omit() %>% unique()
  
  # Filtrar URLs que contienen la palabra "propiedad"
  url_subset <- url_full[grepl("propiedad", url_full)]
  
  # Guardar las URLs en un archivo de texto
  writeLines(url_subset, con = file_name)
}

# Generar los archivos de texto para cada URL
save_url_info(urls[1], "eduard_martinez_pset4_urls.txt")
save_url_info(urls[2], "fandino_julian_problem_set4_urls.txt")

```
